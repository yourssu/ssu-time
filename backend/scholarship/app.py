import asyncio
import logging
from datetime import datetime
from typing import List
import json

from config import DEFAULT_CRAWLER_CONFIG
from crawler.scraper import run_single_crawler
from utils.ics_builder import build_ics_from_events, merge_ics_texts

logger = logging.getLogger(__name__)


def _event_key_from_dict(ev: dict) -> tuple:
    dates = sorted(ev.get('dates', []))
    if not dates:
        return (ev.get('title', ''), '', '')
    start = dates[0]
    end = dates[-1]
    return (ev.get('title', ''), f"{start[0]:04d}-{start[1]:02d}-{start[2]:02d}", f"{end[0]:04d}-{end[1]:02d}-{end[2]:02d}")


async def run_many_crawlers_and_generate_ics(crawler_configs: List[dict]) -> tuple[str, list[dict]]:
    # ì„œë¡œ ë…ë¦½ëœ ë„¤íŠ¸ì›Œí¬ ì‘ì—…ì´ë¯€ë¡œ ë³‘ë ¬ ì‹¤í–‰
    results = await asyncio.gather(*[run_single_crawler(cfg) for cfg in crawler_configs])
    all_events = []
    all_misses = []
    for r in results:
        all_events.extend(r.get('events', []))
        all_misses.extend(r.get('misses', []))
    # ì¤‘ë³µ ì œê±°: (title, startDate, endDate) í‚¤ ê¸°ì¤€
    deduped = []
    seen = set()
    for ev in all_events:
        k = _event_key_from_dict(ev)
        if k in seen:
            continue
        seen.add(k)
        deduped.append(ev)
    logger.info(f"ì´ ì´ë²¤íŠ¸ ìˆ˜: {len(all_events)} â†’ ì¤‘ë³µ ì œê±° í›„ {len(deduped)} / ë‚ ì§œ ë¯¸íƒì§€: {len(all_misses)}")
    return build_ics_from_events(deduped), all_misses


async def main():
    logger.info("ğŸš€ í•¨ìˆ˜ ê¸°ë°˜ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
    start = datetime.now()
    ics_text_new, misses = await run_many_crawlers_and_generate_ics([DEFAULT_CRAWLER_CONFIG])

    # ê¸°ì¡´ íŒŒì¼ì´ ìˆìœ¼ë©´ ë³‘í•©(ì¤‘ë³µ ì œê±°)
    try:
        with open('scholarships.ics', 'r', encoding='utf-8') as f:
            existing = f.read()
    except FileNotFoundError:
        existing = ''

    merged = merge_ics_texts(existing, ics_text_new)
    with open('scholarships.ics', 'w', encoding='utf-8') as f:
        f.write(merged)

    # ë‚ ì§œ ë¯¸íƒì§€ ëª©ë¡ ì €ì¥ (JSON ë°°ì—´)
    if misses:
        with open('missing_dates.json', 'w', encoding='utf-8') as f:
            json.dump([
                {
                    'title': m.get('title'),
                    'url': m.get('url'),
                    'reason': m.get('message') or 'ë‚ ì§œë¥¼ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤',
                } for m in misses
            ], f, ensure_ascii=False, indent=2)
    elapsed = (datetime.now() - start).total_seconds()
    logger.info("ğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
    logger.info(f"â± ì´ ì†Œìš” ì‹œê°„: {elapsed:.2f}ì´ˆ")


if __name__ == '__main__':
    asyncio.run(main())
